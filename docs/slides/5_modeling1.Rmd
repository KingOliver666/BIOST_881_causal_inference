---
title: "L5: Modeling Part 1"
author: "Jean Morrison"
institute: "University of Michigan"
date: "2022-24-10 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

$\newcommand{\ci}{\perp\!\!\!\perp}$

# Modeling

- So far we have learned two strategies for estimating $E[Y(a)]$ from data when we need to adjust for
confounding or selection bias. 

- In IP weighting we re-weight our observations by $1/P[A = A_i \vert L_i]$, the probability that 
they received the treatment that they got given confounderts $L_i$. 

- In standardization, we first stratify the data by $L$, compute $E[Y \vert A = a, L]$ within each stratum, and then take a weighted sum of these averages. 

- In both of these approaches, we have stayed totally non-parametric so far.

---
# Non-Parametric Models

- Our methods so far have been *non-parametric* because we haven't had to make any assumptions about the 
form of quantities like $E[Y \vert A, L]$.

- We just estimate one value for every level of $A$ and $L$.

- This is also called a *saturated* model. 

- With this strategy, we can be sure that the true probability distribution is within th class we are searching.

- So with enough data and the correct model, we will always converge to the true effect. 

---

# Limitations of Non-Parametric Models

- Non-parametric models are limited in the complexity of data they can handle.

- We can only handle variables with a small number of discrete levels. 

- We will need to make some assumptions to handle more complex data. 

- The validity of our inference will depend on these assumptions being correct. 

- But many parametric models are still flexible enough to do well. 


---
# NHEFS Data

- Cigarette smokers aged 25-74 were recruited around 1971 and given a survey. 

- Ten years later, participants were given a followup survey. 

- We are interested in estimating the effect of quitting smoking on weight change on the additive scale. 


---

# Variables

- Our exposure is binary (whether or not a person quit smoking between the first and second survey).

- The outcome is the change in weight in kg. 


- What other features you would want to know in order to estimate the causal effect. 

--

- For now we will assume that the following variables are sufficient: 

+ Sex (0: male, 1: female)
+ Age (in years)
+ Race (0: white, 1: other)
+ Education (5 categories)
+ Intensity of smoking (cigarettes per day)
+ Duration of smoking (years)
+ Physical activity in daily life (3 categories)
+ Recreational exercise (3 categories)
+ Weight (kg)

---
# Limitations

- We are only analyzing individuals who responded to the second survey.

  + For individuals who did not respond, we have neither exposure nor outcome data. 
  + We will talk more about this later but for now are ignoring it. 

- Individuals may have quit at different times. 

  + We could think of $A$ as a time-varying treatment (coming up later). 

---
# Simple Analysis

- The simplest analysis is to simply compare the mean weight change between quitters and non-quitters. 


```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
dat.u <- read_csv("../../data/nhefs.csv") 
dat <- dat.u %>%
       filter(!is.na(wt82))
tab <- dat %>% group_by(qsmk) %>% filter(!is.na(wt82_71)) %>% summarize( mean_wt = mean(wt82_71, na.rm=T), n = n())

kable(tab, col.names = c( "Quit Smoking", "Average Weight Change", "N"), 
      format = 'html', row.names = FALSE, digits = 2) 
```


---
# Comparing Qitters and Non-Qitters

```{r, echo = FALSE}

tab1 <- dat %>% group_by(qsmk) %>%
        summarize(age_m = mean(age), age_sd = sd(age), 
                  fem_n = sum(sex), fem_p = mean(sex)*100, 
                  race_n = sum(race), race_p = mean(race), 
                  ed5_n = sum(education==5), ed5_p = mean(education==5)*100,
                  wt_m = mean(wt71), wt_sd = sd(wt71),
                  smokeint_m = mean(smokeintensity), smokeint_sd = sd(smokeintensity), 
                  smokeyrs_m = mean(smokeyrs), smokeyrs_sd = sd(smokeyrs), 
                  ex2_n = sum(exercise==2), ex2_p = mean(exercise==2)*100, 
                  act2_n = sum(active ==2), act2_p = mean(active==2)*100)

tab1_f <- tab1 %>% 
          mutate(age = paste0(round(age_m, digits = 1), " (", round(age_sd, digits = 1), ")"), 
                 female = paste0(round(fem_p, digits = 1), "% (", fem_n, ")"), 
                 nonwhite = paste0(round(race_p, digits = 1), "% (", race_n, ")"), 
                 university = paste0(round(ed5_p, digits = 1), "% (", ed5_n, ")"), 
                 weight71 = paste0(round(wt_m, digits = 1), " (", round(wt_sd, digits = 1), ")"), 
                 int = paste0(round(smokeint_m, digits = 1), " (", round(smokeint_sd, digits = 1), ")"), 
                 yrs = paste0(round(smokeyrs_m, digits = 1), " (", round(smokeyrs_sd, digits = 1), ")"), 
                 ex2 = paste0(round(ex2_p, digits = 1), "% (", ex2_n, ")"),
                 act2 = paste0(round(act2_p, digits = 1), "% (", act2_n, ")")
                 ) %>%
          select(qsmk, age, female, nonwhite, university, weight71, int, yrs, ex2, act2) %>% 
          gather("variable", "value", -qsmk) %>% spread(qsmk, value) 

tab1_f$variable <- recode(tab1_f$variable, "act2" = "Inactive life", 
                          "age" = "Age, years",
                          "nonwhite"="Non-White", 
                          "weight71" = "Weight, kg", 
                          "female" = "Female", 
                          "int" = "Cigarettes/day", 
                          "university" = "College",
                           "yrs" = "Years smoking", 
                          "ex2" = "Little exercise")
kable(tab1_f, col.names = c("Variable", "Did Not Quit Smoking", "Quit Smoking"), 
      format = 'html', row.names = FALSE, digits = 2) 
```

---
# Inverse Probability Weighting

- Recall, the goal of IP weighting is to create a pseudo-population in which confounders $L$ and
treatment $A$ are independent. 

- If $Y(a) \ci A \vert L$, then in the pseudo-population, $E_{ps}[Y(a)]  = E[Y \vert A]$. 

- In the pseudo-population, association is causation, 
$$
E[Y(1)]-E[Y(0)] = E[Y \vert A = 1] - E[Y \vert A = 0]
$$

---
# Estimating Weights

- In our data, $L$ is a vector of nine measurements. 

- We cannot compute $P[A = 1 \vert L ]$ among every (or any) strata of $L$ because every
participant has their own unique value of $L$. 

- We have to fit a parametric model. 

- What would you do? 

---

# Weights Estimated by Logistic Regression 

- We will use a logistic regression model to predict $P[A \vert L]$

- The goal is to get the best possible estimate of the probability of treatment so 
we should fit a flexible model. 

```{r}
fit <- glm(
  qsmk ~ sex + race + age + I(age ^ 2) +
    as.factor(education) + smokeintensity +
    I(smokeintensity ^ 2) + smokeyrs + I(smokeyrs ^ 2) +
    as.factor(exercise) + as.factor(active) + wt71 + I(wt71 ^ 2),
  family = binomial(),
  data = dat
)

dat$w <- ifelse(dat$qsmk == 0, 
                1/(1 - predict(fit, type = "response")), 
                1/(predict(fit, type = "response")))
```

```{r, echo = FALSE}
summary(dat$w)
```

---

# Positivity 

- *Structural violations* of positivity occur when it is impossible for people with some 
levels of confounders to receive a particular level of treatment. 

  + If we have structural violations, we cannot use IP weighting or standardization. 
  + We need to restrict our inference to relevant strata of $L$. 

- *Random violations* of positivity occur when certain combinations of $L$ and $A$ 
are missing from our data by chance. 

  + We are using a parametric model, so we are able to smooth over the zeroes. 
  + We are able to predict for strata that were not observed in our data. 
  + We should be careful about predicting outside the range of the observed data. 


---

# Horvitz-Thompson Estimator

- The *Horvitz-Thompson estimator* is one of two options for estimating $E[Y(a)]$ using our 
estimated weights.

- The Horvitz-Thompson estimator just plugs our estimated weights into our previous formula.

$$
\hat{E}[Y(a)] = \hat{E}\left[\frac{I(A = a)Y}{f(A\vert L)} \right] = \frac{1}{n}\sum_{i = 1}^n I(A_i = a)Y_i W_i
$$

---

# Hajek Estimator

- The *Hajek estimator* fits the linear model 
$$
E[Y \vert A] = \theta_0 + \theta_1 A
$$
by weighted least squares, weighting individuals by our estimated IP weights. 

- It is is equivalent to


$$\frac{ \hat{E}\left[\frac{I(A = a)Y}{f(A\vert L)} \right]}{ \hat{E}\left[\frac{I(A = a)}{f(A\vert L)} \right]} = \frac{\sum_{i = 1}^n I(A_i = a)Y_i W_i}{\sum_{i = 1}^n I(A_i = a) W_i}$$
---

# Hajek Estimator

- Hajek estimator is unbiased for 

$$\frac{ E\left[\frac{I(A = a)Y}{f(A\vert L)} \right]}{ E\left[\frac{I(A = a)}{f(A\vert L)} \right]}$$
- Under positivity, 

$$
E\left[\frac{I(A = a)}{f(A\vert L)} \right] = 1
$$

- Guaranteed to be between 0 and 1 for dichotomous outcomes. 

- If positivity does not hold, the Hajek estimator is unbiased for $E[Y(a) \vert L \in Q(a)]$, where $Q(a)$ is the set of values of $l$ such that $P[A = a \vert L = l] > 0$. 

- Without positivity, the risk difference estimated by the Hajek estimator does not have a causal interpretation. 
---

# IP Weighted Effect Estimate in NHEFS

```{r}
lm(wt82_71 ~ qsmk, data = dat, weights  = w)
```

- We estimate that quitting smoking leads to a 3.4 kg increase in weight gain. 

---
# Estimating the Variance of the Estimate

- The raw variance estimate from weighted least squares does not account for uncertainty in the weights. 

- We need to account for having estimated the IP weights. 

- Options: 

  1. Derive the variance anlytically
  2. Non-parametic bootstrapping (more details later)
  3. Robust variance estimate using an independent working correlation matrix. 
  
- Option 3 is conservative but easier than options 1 and 2. 

---

# Variance Estiamte in the NHEFS Data

```{r}
library("geepack")
msm.w <- geeglm(
  wt82_71 ~ qsmk,
  data = dat,
  weights = w,
  id = seqn,
  corstr = "independence"
)
beta <- coef(msm.w)
SE <- coef(summary(msm.w))[, 2]
lcl <- beta - qnorm(0.975) * SE
ucl <- beta + qnorm(0.975) * SE
cbind(beta, lcl, ucl)
```

---
# Stabilized Weights

- Using weights $W^{A} = 1/f(A \vert L)$ we create a pseudo-population in which (heuristically), each person is matched by someone exactly like them who received the opposite treatment. 


- Our pseudo-population is twice as big as our actual sample so the mean of $W^A$ is 2. 

- In the pseudo-population, the frequency of treatment $A = 1$ is 50%. 

- We could have created other pseudo-populations. 

---
# Stabilized Weights

- Our requirements are that, in the pseudo-population, probability of treatment is independent of $L$. 

- But different people could have different probabilities of treatment. 


- To create stabilized weights, we construct a pseudo-population in which the probability of receiving 
each treatment is the same as the frequency of the treatment in our original sample. 

$$
SW^A = \frac{f(A)}{f(A \vert L)}
$$
---

# Stabilized Weights

- In our data, $A$ is binary, so we can compute $f(1)$ as the proportion of quitters in the data. 

```{r}
p1 <- mean(dat$qsmk); p1
```

- In the pseudo-population created by the stabilized weights, person in our data set corresponds to  26% of a treated person and 74% of an untreated person. 

```{r}
dat <- dat %>% mutate(pa = ifelse(dat$qsmk == 0, 1-p1,  p1),
              sw= pa*w)
summary(dat$sw)
```

- The expected value of $SW^A$ is 1 because the pseudo-population is the same size as
the observed data. 

---

# Estimation Using the Stabilized Weights

```{r}
msm.w <- geeglm(
  wt82_71 ~ qsmk,
  data = dat,
  weights = sw,
  id = seqn,
  corstr = "independence"
)
beta <- coef(msm.w)
SE <- coef(summary(msm.w))[, 2]
lcl <- beta - qnorm(0.975) * SE
ucl <- beta + qnorm(0.975) * SE
cbind(beta, lcl, ucl)
```

- These are exactly the results we saw with the unstabilized weights. 

---
# Why Use Stabilized Weights

- Differences between stabilized and non-stabilized weights only occur when the model for $E[Y \vert A]$ is not saturated. 


- When the model is not saturated, stabilized weights typically result in greater efficiency. 

---

# Marginal Structural Models

- In the IP weighting strategy we create a population in which $A$ is indepedent of $L$ and then fit the model 
$$E_{ps}[Y \vert A ] = \theta_0 + \theta_1 A$$

- If we believe our conditional exchangeability assumption $Y(a) \ci A \vert L$, then in the pseudo-population, $E_{ps}[Y \vert A] = E[Y(a)]$. 

- So the parameter $\theta_1$ is interpretable as the causal risk difference. 

- We have proposed a model for the average counterfactual: 

$$E[Y(a)] = \beta_0 + \beta_1 a$$
- This model is *marginal* because we have marginalized (averaged) over the values of all
other covariates. 

- It is structural because it is a model for a counterfactual $E[Y(a)]$ rather than 
a probability distribution $E[Y \vert A]$.

---
# Modeling Continuous Treatments

- Because out treatment was binary, we could construct a saturated model. 

- For continuous (or highly polytomous) variables, we can't do that and will have to use 
a parametric model instead. 

- In the NHEFS data, let $A$ be the change in smoking intensity ( $A = 10$ indicates that a person increased their smoking by 10 cigarettes).

- We will limit to only those who smoked 25 or fewere cigarettes per day at baseline ( $N = 1,162$ )

- We can propose a model for our *dose-response curve*

$$
E[Y(a)] = \beta_0  + \beta_1 a + \beta_2 a^2
$$

---
# Estimating Weights for the Continuous Treatment

- To use stabilized weights, we need to estimate $f(A \vert L)$ and $f(A)$. 

- Both of these are PDFs which are hard to estimate!

- HR propose a linear model for $E[A \vert L]$ and then assume that $f(A \vert L)$ is
normal with mean $\hat{E}[A \vert L]$ and constant variance $\sigma^2$.
  - We can use the variance of the residuals as an estimate of $\sigma^2$.
  
- We assume that $f(A)$ is normal. 

- These assumptions are almost certainly wrong. 

- IP weighted estimates of continuous variables can be very sensitive to choice of weights.

---
# Estimating Weights for the Continuous Treatment

- Estimating $f(A \vert L)$
```{r}
dat.s <- filter(dat, smokeintensity <= 25)
fal_fit<- lm(
  smkintensity82_71 ~ as.factor(sex) +
    as.factor(race) + age + I(age ^ 2) +
    as.factor(education) + smokeintensity + I(smokeintensity ^ 2) +
    smokeyrs + I(smokeyrs ^ 2) + as.factor(exercise) + as.factor(active) + wt71 +
    I(wt71 ^ 2), data = dat.s)
fal_mean <- predict(fal_fit, type = "response")
fal  <- dnorm(dat.s$smkintensity82_71, fal_mean,
        summary(fal_fit)$sigma)
```

---
# Estimating Weights for the Continuous Treatment

- Estimating $f(A)$
```{r}
fa_fit <- lm(smkintensity82_71 ~ 1, data = dat.s)
fa_mean <- predict(fa_fit, type = "response")
fa <- dnorm(dat.s$smkintensity82_71, fa_mean,
            summary(fa_fit)$sigma)
dat.s$sw_cont <- fa / fal
dat.s$w_cont <- 1/fal
summary(dat.s$sw_cont)
summary(dat.s$w_cont)
```

---
# Fitting the Continuous Treatment Model

- With standardized weights
```{r}
msm.sw.cont <-geeglm( wt82_71 ~ smkintensity82_71 + I(smkintensity82_71^2),
    data = dat.s, weights = sw_cont, id = seqn, corstr = "independence")
```
```{r, echo = FALSE}
beta <- coef(msm.sw.cont)
SE <- coef(summary(msm.sw.cont))[, 2]
lcl <- beta - qnorm(0.975) * SE
ucl <- beta + qnorm(0.975) * SE
cbind(beta, lcl, ucl)
```

- With non-standardized weights

```{r}
msm.w.cont <- geeglm( wt82_71 ~ smkintensity82_71 + I(smkintensity82_71^2),
    data = dat.s, weights = w_cont, id = seqn, corstr = "independence")
```

```{r,echo = FALSE}
beta <- coef(msm.w.cont)
SE <- coef(summary(msm.w.cont))[, 2]
lcl <- beta - qnorm(0.975) * SE
ucl <- beta + qnorm(0.975) * SE
cbind(beta, lcl, ucl)
```
---
# Issues with the Continuous Treatment Model

---
# Effect Modification in Marginal Structural Models

- Suppose we are interested in estimating the causal effect within strata of $V$. 

- We can propose a marginal structural model that includes $V$

$$
E[Y(a) \vert V] = \beta_0 + \beta_1 a + \beta_2 a V + \beta_3 V
$$

- If $V$ and $A$ are both dichotomous, this model is saturated. 

- It is not really fully marginal any more, but we still call it a marginal structural model. 

---
# Interpreting Parameters in the Effect Modification Model

- Suppose that $V$ is sex (0: Male, 1: Female) and $A$ is quitting/not quitting. 

$$
E[Y(a) \vert V] = \beta_0 + \beta_1 a + \beta_2 a V + \beta_3 V
$$

- $\beta_1$ is the causal effect of quitting for men. 

- $\beta_1+ \beta_2$ is the causal effect of quitting for women. 

- What is $\beta_3$?

---
# Stablilized Weights for the EM Model


- We have two choices for stabilized weights

$$
\frac{f(A)}{f(A \vert L)} \qquad \text{or} \qquad  \frac{f(A\vert V)}{f(A \vert L)}
$$

- Which will be more efficient?

--

- Conditioning on $V$ in the numerator will make the numerator closer to the denominator. 

- The second choice of weights will be less variable. 

- So the second choice of weights is more efficient. 

---
# Stablilized Weights for the EM Model

- We need to compute $f(A \vert V)$ in our data. 

- We can just use the stratified means. 

```{r}
fav_fit <- lm(qsmk ~ sex, data = dat)
dat$pav <- ifelse(dat$qsmk == 0,
                  1-predict(fav_fit, type = "response"), 
                  predict(fav_fit, type = "response"))
dat <- mutate(dat, sw_em = pav*w)
summary(dat$sw_em)
```

---
# Fitting the Effect Modification Model

```{r}
msm.emm <- geeglm(
  wt82_71 ~ qsmk*sex, data = dat, weights = sw_em, id = seqn,
  corstr = "independence")
beta <- coef(msm.emm)
SE <- coef(summary(msm.emm))[, 2]
lcl <- beta - qnorm(0.975) * SE
ucl <- beta + qnorm(0.975) * SE
cbind(beta, lcl, ucl)
```

- We have no strong evidence of effect modificaiton by sex. 
---

# Unweighted Regression vs IP Weighting 

- What if we had conditioned on every variable in $L$? 

- The stabilized weights would be 1, so we would just be running a regular 
regression. 

- If we believe in the parametric model for $E[Y(a) \vert L]$, then the regression coefficient
is interpretable as a causal parameter. 

- Using the marginal model with IP weighting, we only need to trust that our estimates of $f(A \vert L)$ are accurate. 

---

# Censoring and Missing Data With IP Weights

- In our analysis, we have removed 63 individuals who filled in the survey in 1982 but 
who have a missing value for weight. 


```{r, echo = FALSE}
tab <- dat.u %>% group_by(qsmk) %>% summarize( nmiss = sum(is.na(wt82)), pmiss = round(mean(is.na(wt82))*100, digits = 1))

kable(tab, col.names = c( "Quit Smoking", "Number Missing", "Percent Missing"), 
      format = 'html', row.names = FALSE, digits = 2) 

```

- We have more missing data for people who quit smoking than people who did not, so we could have selection bias. 


---

# Weights for Censoring

- We learned in L4 that to correct for selection bias, we need to find a set of variables $L_c$ such that $Y(C = 0) \ci C \vert L_c$. 

- We then need to estimate $W^C = 1/P[C = 0 \vert A, L_c]$, or the stabilized version $$SW^C = \frac{P[C = 0 \vert A]}{P[C = 0 \vert A, L_c]}$$

- We will use the same set of variables, we used to compute weights for confounding. 

- The total weights will be $SW^C \cdot SW^A$


---

# Weights for Censoring

- Estimating $f(C=0 \vert A, L)$ using a logistic model

```{r}
dat.u$cens <- ifelse(is.na(dat.u$wt82), 1, 0)
fcal_fit <- glm(
  cens ~ as.factor(qsmk) + as.factor(sex) +
    as.factor(race) + age + I(age ^ 2) +
    as.factor(education) + smokeintensity +
    I(smokeintensity ^ 2) + smokeyrs + I(smokeyrs ^ 2) +
    as.factor(exercise) + as.factor(active) + wt71 + I(wt71 ^ 2),
  family = binomial(),
  data = dat.u
)
fc0al <- 1 - predict(fcal_fit, type = "response")
```

---

# Weights for Censoring

- Estimating $f(C = 0 \vert A)$ using the average within levels of $A$. 

```{r}
fca_fit <-glm(cens ~ as.factor(qsmk), family = binomial(), data = dat.u)
fc0a <- 1 - predict(fca_fit, type = "response")
sw.c <- fc0a/fc0al
dat$sw.c <- sw.c[!is.na(dat.u$wt82)]
dat$sw <- dat$sw*dat$sw.c
summary(dat$sw.c)
summary(dat$sw)
```
---
# Fitting with the Combined Weights

```{r}
msm.sw <- geeglm(wt82_71 ~ qsmk, data = dat,weights = sw,
  id = seqn,  corstr = "independence")
beta <- coef(msm.sw)
SE <- coef(summary(msm.sw))[, 2]
lcl <- beta - qnorm(0.975) * SE
ucl <- beta + qnorm(0.975) * SE
cbind(beta, lcl, ucl)
```

---

# Standardization

- Recall from previous lectures (and homework) that IP weighting and standardization 
were equivalent when we could be fully non-parametric. 

- Our standardization formula is 

$$
E[Y(a)] = \sum_l P[Y \vert A = a, L = l]P[L = l]
$$
- In our example, $L$ is continuous, so we need to integrate

$$
E[Y(a)] = \int_l P[Y \vert A = a, L = l]f(l) dl
$$

---

# Outcome Modeling

- In the standardization formula, we need to estimate $P[Y \vert A = a, L = l]$.


---

# Standardizing

- Fortunately, we do not need to compute $f(l)$.


- Using the double expectation, we can write
$$
 \int_l P[Y \vert A = a, L = l]f(l)dl = E\left[ E\left[Y \vert A = a, L = l \right]\right]
$$
- So we can estimate the standardized mean as

$$
\frac{1}{n}\sum_{i=1}^n\hat{E}[Y \vert A = a, L_i]
$$
---

# Standardizing

- For each person, we need to compute two values $\hat{E}[Y \vert A = 0, L_i]$ and $\hat{E}[Y\vert A = 1, L_i]$

- These are predicted values that we can get out of our previous regression. 


---

# Estimating the Variance 

- To compute the variance of the estimate, we use bootstrapping. 

---

# Effect Modification in Standardization

---

# IP Weighting vs Standardization

---

# Doubly Robust Estimators


---

# g-formula





# Jean Notes

- Conditionality prinicipal (Ch. 10)
