---
title: "L17: Odds and Ends"
author: "Jean Morrison"
institute: "University of Michigan"
date: "Lecture on 2023-04-12\n (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
$\newcommand{\ci}{\perp\!\!\!\perp}$
```{r xaringanthemer, include=FALSE, warning=FALSE, echo = FALSE}
library(xaringanthemer)
style_mono_accent(
  link_color = "#ea8a1a",
  base_color = "#110566",
 # header_font_google = google_font("Josefin Sans"),
 # text_font_google   = google_font("Montserrat", "300", "300i"),
 # code_font_google   = google_font("Fira Mono")
)
xaringanExtra::use_tile_view()
xaringanExtra::use_search(show_icon = TRUE)
xaringanExtra::use_panelset()
library(tidyverse)
library(DiagrammeR)
library(knitr)
library(kableExtra)
```

## Plan for Today

1. Last mediation slides 

1. Review of some homework problems

1. A few slides about cross-fitting

1. Open question time
---

## Homework 2, Question 2

- This problem was about selection bias. In part (a) we have this graph:

<center>

```{r, echo = FALSE, out.width='40%', fig.height = 4, fig.align='left'}
g1 <- create_node_df(n = 4, label = c("L", "A", "Y", "C"), 
                     fontname = "Helvetica", 
                     fontsize = 10, 
                     width = 0.3, 
                      fillcolor = "white", 
                     color = "black", 
                     shape = c(rep("circle", 3), "square"),
                     x = c(2, 1, 3, 1)*0.5, 
                     y = c(1, 0, 0, -1)*0.7)
g1_edge <- create_edge_df(from = c( 1, 1, 2, 2), to = c(2, 3, 3, 4),
                          color = "black", 
                          )
g1_graph <- create_graph(nodes_df = g1, edges_df = g1_edge)

render_graph(g1_graph)
```
</center>

- The easiest parts of this question is (i) If there is no censoring, how do we identify the effect of $A$ on $Y$. 

- With no missing data, we can condition on $L$ and use the standard G-formula.

---

## Homework 2, Homework 2, Question 2

<center>

```{r, echo = FALSE, out.width='40%', fig.height = 3, fig.align='left'}
render_graph(g1_graph)
```
</center>

- If only $Y$ is missing for censored units, the g-formula still works:

$$
\begin{split}
E[Y(a)] =& \sum_l P[Y \vert A = a, L=l] P[L = l]\\
= & \sum_l P[Y \vert A = a, L=l, C = 0] P[L = l]
\end{split}
$$
- The first line is the usual g-formula.

- The second line follows because $Y \ci C \vert A, L$, so we can condition on $C$ without changing the value of the expectation. 

---

## Homework 2, Homework 2, Question 2

<center>

```{r, echo = FALSE, out.width='40%', fig.height = 3, fig.align='left'}
render_graph(g1_graph)
```
</center>

- However, if everything is missing for censored units, we are out of luck. 

- The G-formula for this problem includes a term $P[L = l]$, which we can't estimate if $L$ is missing when a unit is censored. 

- If we only had $A$ for censored units, we could use IP weighting, weighting each non-censored unit by $1/P[C = 0 \vert A]$.


---

## Homework 2, Homework 2, Question 2

- Notice that this probelm is different from the no-confounding case. 

<center>

```{r, echo = FALSE, out.width='40%', fig.height = 2, fig.align='left'}
g2 <- g1[-1,]
g2_edge <-g1_edge %>% filter(from != 1)
g2_graph <- create_graph(nodes_df = g2, edges_df = g2_edge)

render_graph(g2_graph)
```
</center>

- In this graph, the causal effect is identifiable even if all data is missing for censored units because there is no confounder to average over. 


---

## Homework 2, Homework 2, Question 2

- Part (c) of this problem was about this graph:

<center>

```{r, echo = FALSE, out.width='40%', fig.height = 4, fig.align='left'}
g3 <- create_node_df(n = 5, label = c("L", "A", "Y", "C", "W"), 
                     fontname = "Helvetica", 
                     fontsize = 10, 
                     width = 0.3, 
                      fillcolor = "white", 
                     color = "black", 
                     shape = c(rep("circle", 3), "square", "circle"),
                     x = c(3, 1, 4, 3, 2)*0.5, 
                     y = c(-1, 0, 0, 1, 1)*0.4)
g3_edge <- create_edge_df(from = c( 1, 1, 2, 5, 2), to = c(5, 3, 5, 4, 3),
                          color = "black", 
                          )
g3_graph <- create_graph(nodes_df = g3, edges_df = g3_edge)

render_graph(g3_graph)
```
</center>

- This problem is a little interesting because there are multiple ways to identify $E[Y(a)]$ when only some data is missing for censored units. 

- Option 1: Calculate IP weights, $1/P[C = 0 \vert W]$.


---

## Homework 2, Homework 2, Question 2

<center>

```{r, echo = FALSE, out.width='40%', fig.height = 2.5, fig.align='left'}
render_graph(g3_graph)
```
</center>

- Option 2: Condition on $L$

$$
\begin{split}
E[Y(a)] =& E[Y \vert A = a] \\
= & \sum_l E[Y \vert A = a, L = l]P[L = l]\\
= & \sum_l E[Y \vert A = a, L = l, C = 0]P[L = l]
\end{split}
$$
- We use the g-formula in the second line with the justifiction that $Y(a)\ci A \vert L$.

---

## Homework 2, Homework 2, Question 2

<center>

```{r, echo = FALSE, out.width='40%', fig.height = 2.5, fig.align='left'}
render_graph(g3_graph)
```
</center>

- Option 3: Average over $P[W \vert A]$

$$
\begin{split}
E[Y(a)] = &E[Y \vert A = a] \\
= & \sum_w E[Y \vert A = a, W = w]p[W =w \vert A = a]\\
= & \sum_w E[Y \vert A = a, W = w, C = 0]p[W =w \vert A = a]
\end{split}
$$

- The second line is the law of total probability. The third is from $Y \ci C \vert A, W$.

---
## Dealing with Missing Data

- Some folks have had to deal with missing data in their projects. 

- In a real data setting you have a few choices for dealing with missing data, depending on how much and what kind of missingness you have. 

- Weights: If missingness is in only the outcome or in a limited number of covariates, you may be able to use weighting. 

- To use weighting, set everyone missing any values to "missing" ( $C = 1$ ).

- Weighting is a good choice if, after censoring all individuals with missing values, 
  + You have a set of covariates $L_1$ such that $Y \ci C \vert L_1$. 
  + There is no missingness in $L_1$. 
  
  
---
## Dealing with Missing Data: Weighting

- If you are computing the IP weighted estimate, you would multiple the censoring weights and the confounding weights together. 

- You can use weights with G-computation, as well. 

- To do this, you would fit the outcome model in complete cases. 

- Then at standardization step, you compute the weighted average

$$E[Y(a)] = \frac{\sum_{i=1}^n w_i \hat{Y}_{a,i}}{\sum_{i=1}^n w_i}$$

- This works with survey weights as well as censoring weights. 

---
## Dealing with Missing Data: Weighting

- The weighted average trick works for any estimator that is a substitution (aka standardization) estimator. 

- This means that it also works for TMLE. 

- In the last step of TMLE, after doing the one step update, we standardize over the updated outcome model. 
- With survey or censoring weights, we could compute the weighted average instead of the sample average.


---
## Dealing with Missing Data: G-Computation

- Another option using G-computation directly works when you only have missing data in $Y$ and you have a set of covariates $L = L_1 \cup L_2$ such that:
  + $Y \ci C \vert L_1$
  + $Y(a) \ci A \vert L_1, L_2$
  
- In this case, we can compute the outcome regression using complete-cases only conditional on $L_1$ and $L_2$.

- In the last step, we standardize over everyone, not just the complete cases. 

- This doesn't work if there are missing value of covariates.

---
## Dealing with Missing Data: Multiple Imputation

- The last option is multiple imputation. 

- This may be the only option if you have sporadic missingness over many covariates. 

- In multiple imputation, you estimate the conditional distribution of each variable given the other variables iteratively. 

- At the end you should be able to sample values for the missing cells in your data. Sample $M$ "filled-in" data sets. 

- For each data set, compute the estimate you would have computed if there had been no missing data. 

- Your final estimate is the average of the $M$ estimates. The variance is close to being the average of the variances, there is a small correction factor.

---
## Homework 3, Question 1

- In this question, you calculated several estimates of the effect of quitting smoking on blood pressure. 

- A few things you might have noticed in this problem: 

- Fitting the subclassification regression estimator may have been a pain. You might have gotten lots of warnings about regression estimates not converging etc. 

- This can be a major limitation of the SR estimate, especially when there are many catgorical predictors. 

- You might have found that when you bootstrap, you occaisionally get wild values from the Scharfstein and Bang and Robins estimates. 
  + This happens when there are very large weights. Because the weights are in the regression model, these act as very high leverage points. 
  
---
## What to Do About Giant Weights

- If some of your IP weights are enormous, that means that for some people $P[A = A_i \vert L_i]$ is very very small. 

- This can happen if you have near positivity issues, i.e. there are regions of covariate space that have very few treated or very few control units. 

- Scientifically, this could indicate a structural positivity issue so it is worth thinking about whether that could be the case. 

- You may determine that you don't think there is a structural positivity issue but there is a practical issue with trying to fit some of our estimators. 

---
## What to Do About Giant Weights

- Option 1: Subset the population. 

- This solution works if you are able to explain your large weights with one or a small number of covariates.

- For example, in an on-going project, we found that individuals who did not have English as a first language had close to zero (but not exactly zero) change of enrolling in our study. 

- We didn't think that it made sense to extrapolate about all non-enrolled non-English speaking individuals from the very small number who were enrolled, so we excluded these individual. 

- The downside of this choice is that it changes the population that we are making inference about. 

---
## What to Do About Giant Weights

- Option 2: Weight truncation. 

- In this option, you simply "trim off" (aka remove) individuals at the tails of the weight distribution. 

- For example, you could remove all individuals with weight below the 1st percentile or above the 99th precentile. 

- This method has the same downside as option 1, the population has changed. 

- However, it is worse in this method because we can't concisely describe how the population has changed. 


---
## What to Do About Giant Weights

- Option 3: Weight stabilization. 

- We saw previously that stabilizing weights can dramatically reduce variance. To stabilize weights, we compute

$$W_i^{S} = \frac{P[A=A_i \vert L_s=L_{s,i}]}{P [A = A_i \vert L = L_i, L_s = L_{s,i}]}$$
- Here $L_s$ is a subset of covariates. $L_s$ could be the empty set so the numerator is just $P[A = A_i]$.

- If you are using IP weighting + marginal structural model, your marginal structural model must now condition on $L_s$. 

- This means that it may be easier to misspecify the marginal structural model.

- We saw an example of this in [Cole et al. (2007)](https://pubmed.ncbi.nlm.nih.gov/17478436/).


---
## What to Do About Giant Weights

- Option 4: Use a substititution estimator. 

- G-computation and TMLE are both substitution estimators, meaning that we average over prediction of $\hat{Y}_{a,i}$. 

- These methods are much more stable (lower variance) in the face of large weights. 

- However, when some regions of covariate space are sparse in treated or control units, G-computation can suffer from extrapolation bias.

- So just because your estimate fit and gave a reasonable, low variance, estimate, this doesn't mean it is an accurate estimate. 

- TMLE will generally be more accurate than plain-vanilla G-computation. 

---
## Homework 3, Question 2

- There were a few take-aways I was hoping you would find in this problem. 

- In the first comparison of multiple Hajek estimators, we would expect that the estimator with incorrect PS model would not be consistent (large bias). 

- We would also expect that using the estimated propensity scores with the correct model would be better than knowing the true propensity scores. 

- In the second comparison, G-estimation and full exact matching had the lowest bias. 

- Full exact matching is equivalent to G-computation with a saturated mean model. 

- However, G-estimatino had much lower variance than the full, exact matching estimate, probably due to small group sizes for the matching estimator. 

---
# A Few Slides About Sample Splitting

---

## Cross-Fitting/Sample Splitting

- For these slides, remember back to our lectures on machine learning and TMLE. 

- In that lecture, we learned that TMLE and other double robust methods are consistent under weaker conditions than singly robust methods. 

- In marticular, we can use machine learning methods that converge slower than $\sqrt{N}$ and still get $\sqrt{N}$-convergence from our causal parameter estimate. 

- This happens if both exposure and outcome estimates are consistent and the product of their convergence rates is $<\sqrt{N}$. For example, they could both converge at rate $N^{1/4}$.

---

## Cross-Fitting/Sample Splitting

- There was another condition to achieve $\sqrt{N}$-consistency that we didn't mention in that lecture. 

- This condition is that methods used to estimate the exposure model and outcome model cannot be "too adaptive" (formally, they need to satisfy Donsker conditions which say that the class of distributions in the model is not too complex).

- If the ML methods are too adaptive, there will be too much over-fitting.

- Many ML estiamtors don't satisfy these conditions. For example, many random forest/tree-based methods. 

- Sample splitting is a very intuitive and effective way to overcome this limitation. 

---

## Cross-Fitting/Sample Splitting


- First, we consider the classic "train-test" machine learning strategy. 

- We divide our data into two pieces (lets say in half). 

- In the first half we train a ML model and in the second half we use the fitted model to estimate something. Often this is model performance.

- For one of our double robust estimators we could do the same thing. 

- Fit the outcome and exposure model in one half and then compute the estimate of $E[Y(a)]$ in the second half. 

- This would solve the overfitting problem.

---

## Cross-Fitting/Sample Splitting

- However, the train-test strategy is unsatisfying because now we only have half the data to train the model and only have the data to estimate with, so we will have a higher variance. 

- We can get around this problem by doing the procedure twice. 

- After training in the first half and estimating in the second, we swap and train in the second half and estimate in the first.

- The two estimates are close to indpendent so we can get a final estimate by averaging them. 

- This gives a final estimator with very nearly the same efficiency as an estimator fit in the whole data but without the overfitting bias. 

- In practice, it is generally better to split the data into $K$-folds, using each fold as the estimation fold in turn. 

- This is extremely similar to cross-validation. 

---

## Cross-Fitting/Sample Splitting

- Sample splitting is employed by the CV-TMLE method. 

- It is also strongly associated with double machine learning (DML). 

- DML is a double robust estimation method that is very similar (in some cases equivalent) to the AIPW (RRZ) estimator that we saw earler.

- However, it also incorporates sample splitting for improved compatibility with ML methods. 

- In either case, using sample splitting means that a wider range of ML methods can be used for estimation which can improve performance. 

- DML Reference: [Chernozhukov et al. (2018)](https://academic.oup.com/ectj/article/21/1/C1/5056401)

---
# Questions and Last Thoughts

---
## Last Thoughts

- Causal inference and the language of counterfactuals gives a formal framework to describe causality. 

- I hope this framework will be useful to you in your scientific endeavours. 

- Estimating causal effects (as opposed to associations) requires thinking carefully about mechansims and what your data can and can't tell you. 

- The tools we learned in this class are very powerful. Don't be afraid to use them.

- However, always have a critical eye. Remember the check assumptions and identifiability conditions. If one of these is badly violated, don't just keep going. 

- The world of causal inference is very large and growing quickly. There is lots of room for good applications and methodological developments. 

- Please feel free to keep in touch after the class and if you are willing, please fill out the course evaluation.
